import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from sklearn.preprocessing import MinMaxScaler, RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Concatenate, BatchNormalization, Multiply, MultiHeadAttention, Layer, TimeDistributed, Lambda, Conv1D, GRU, RNN
from tensorflow.keras.optimizers import Adam, AdamW, RMSprop, Nadam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
import random
import math
import matplotlib.font_manager as fm
import matplotlib.dates as mdates
from ta.trend import SMAIndicator, MACD
from ta.momentum import RSIIndicator, ROCIndicator
from ta.volatility import BollingerBands
import os
import pickle
import json
import yfinance as yf
import requests
from bs4 import BeautifulSoup
import logging
import psycopg2
from psycopg2.extras import execute_values
from kaggle_secrets import UserSecretsClient
import networkx as nx
from scipy import stats

# TensorFlow ì„¸ì…˜ ì´ˆê¸°í™”
import tensorflow as tf

# ê¸°ì¡´ ì„¸ì…˜ ì •ë¦¬ ë° ë©”ëª¨ë¦¬ í•´ì œ
tf.keras.backend.clear_session()
tf.compat.v1.reset_default_graph()

# GPU ì„¤ì • ë‹¨ìˆœí™”
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'
os.environ['TF_GPU_THREAD_COUNT'] = '4'
os.environ['TF_USE_CUDNN'] = '0'
os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '0'
os.environ['TF_CUDNN_USE_AUTOTUNE'] = '0'
os.environ['TF_CUDNN_DETERMINISTIC'] = '1'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    print(f"GPU ì‚¬ìš© ê°€ëŠ¥: {gpus[0]}")
else:
    print("GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# TensorFlow ìµœì í™” ì„¤ì •
tf.config.optimizer.set_jit(False)
tf.config.optimizer.set_experimental_options({
    "layout_optimizer": True,
    "constant_folding": True,
    "shape_optimization": True,
    "remapping": True,
    "arithmetic_optimization": True,
    "dependency_optimization": True,
    "loop_optimization": True,
    "function_optimization": True,
    "debug_stripper": True,
    "disable_model_pruning": False,
    "scoped_allocator_optimization": True,
    "pin_to_host_optimization": True,
    "implementation_selector": True,
    "auto_mixed_precision": False
})

print("TensorFlow ë²„ì „:", tf.__version__)

# í•œê¸€ í°íŠ¸ ì„¤ì •
try:
    font_list = [f.name for f in fm.fontManager.ttflist]
    for font in ['NanumBarunGothic', 'NanumGothic', 'Malgun Gothic', 'Gulim']:
        if font in font_list:
            plt.rcParams['font.family'] = font
            print(f"í•œê¸€ í°íŠ¸ '{font}' ì‚¬ìš©")
            break
    else:
        print("í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©")

    plt.rcParams['axes.unicode_minus'] = False
except Exception as e:
    print(f"í°íŠ¸ ì„¤ì • ì˜¤ë¥˜: {e}")

# ì¬í˜„ì„± ì„¤ì • ê°•í™”
SEED = 42
os.environ['PYTHONHASHSEED'] = str(SEED)
os.environ['TF_DETERMINISTIC_OPS'] = '1'
os.environ['TF_CUDNN_DETERMINISTIC'] = '1'

# ëª¨ë“  ëœë¤ ì‹œë“œ ì„¤ì •
np.random.seed(SEED)
tf.random.set_seed(SEED)
random.seed(SEED)

# ë°°ì¹˜ í¬ê¸° ì¦ê°€ (GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ ì¡°ì •)
BATCH_SIZE = 128  # 32ì—ì„œ 128ë¡œ ì¦ê°€

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
user_secrets = UserSecretsClient()
DB_HOST = user_secrets.get_secret("DB_HOST")
DB_PORT = user_secrets.get_secret("DB_PORT")
DB_NAME = user_secrets.get_secret("DB_NAME")
DB_USER = user_secrets.get_secret("DB_USER")
DB_PASSWORD = user_secrets.get_secret("DB_PASSWORD")

def get_db_connection():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•¨ìˆ˜"""
    try:
        conn = psycopg2.connect(
            host=DB_HOST,
            port=DB_PORT,
            dbname=DB_NAME,
            user=DB_USER,
            password=DB_PASSWORD
        )
        return conn
    except Exception as e:
        print(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜: {e}")
        return None

def execute_query(query, params=None, fetch=True):
    """ì¿¼ë¦¬ ì‹¤í–‰ í•¨ìˆ˜"""
    conn = None
    try:
        conn = get_db_connection()
        if conn is None:
            raise Exception("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
        
        with conn.cursor() as cur:
            cur.execute(query, params)
            if fetch:
                return cur.fetchall()
            conn.commit()
    except Exception as e:
        print(f"ì¿¼ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        if conn:
            conn.rollback()
        raise
    finally:
        if conn:
            conn.close()

def execute_values_query(query, data):
    """ì—¬ëŸ¬ í–‰ì˜ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì‚½ì…í•˜ëŠ” í•¨ìˆ˜"""
    conn = None
    try:
        conn = get_db_connection()
        if conn is None:
            raise Exception("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
        
        with conn.cursor() as cur:
            execute_values(cur, query, data)
            conn.commit()
    except Exception as e:
        print(f"ë°ì´í„° ì‚½ì… ì˜¤ë¥˜: {e}")
        if conn:
            conn.rollback()
        raise
    finally:
        if conn:
            conn.close()

def execute_transaction(queries):
    """íŠ¸ëœì­ì…˜ ì‹¤í–‰ í•¨ìˆ˜"""
    conn = None
    try:
        conn = get_db_connection()
        if conn is None:
            raise Exception("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
        
        with conn.cursor() as cur:
            for query, params in queries:
                if params is None:
                    cur.execute(query)
                else:
                    cur.execute(query, params)
            conn.commit()
    except Exception as e:
        print(f"íŠ¸ëœì­ì…˜ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        if conn:
            conn.rollback()
        raise
    finally:
        if conn:
            conn.close()

def create_predictions_table():
    """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì €ì¥í•  í…Œì´ë¸” ìƒì„±"""
    queries = [
        ("""
        CREATE TABLE IF NOT EXISTS price_predictions (
            id SERIAL PRIMARY KEY,
            stock_code VARCHAR(10) NOT NULL,
            stock_name VARCHAR(50) NOT NULL,
            prediction_date TIMESTAMPTZ NOT NULL,
            target_date TIMESTAMPTZ NOT NULL,
            predicted_price DECIMAL(10,2) NOT NULL,
            actual_price DECIMAL(10,2),
            prediction_error DECIMAL(10,2),
            created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
        );
        """, None),
        ("CREATE INDEX IF NOT EXISTS idx_price_predictions_date ON price_predictions (prediction_date, target_date);", None),
        ("CREATE INDEX IF NOT EXISTS idx_price_predictions_stock ON price_predictions (stock_code);", None)
    ]
    execute_transaction(queries)
    print("Price predictions table created successfully!")

def create_economic_indicators_table():
    """ê²½ì œì§€í‘œ í…Œì´ë¸” ìƒì„±"""
    queries = [
        ("""
        CREATE TABLE IF NOT EXISTS economic_indicators (
            id SERIAL PRIMARY KEY,
            time TIMESTAMPTZ NOT NULL,
            treasury_10y DECIMAL(10,2),
            dollar_index DECIMAL(10,2),
            usd_krw DECIMAL(10,2),
            korean_bond_10y DECIMAL(10,2),
            created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
        );
        """, None),
        ("CREATE INDEX IF NOT EXISTS idx_economic_indicators_time ON economic_indicators (time);", None)
    ]
    execute_transaction(queries)
    print("Economic indicators table created successfully!")

def create_predicted_prices_table():
    """ì˜ˆì¸¡ ê°€ê²©ì„ ì €ì¥í•  í…Œì´ë¸” ìƒì„±"""
    queries = [
        # ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ
        ("DROP TABLE IF EXISTS predicted_stock_prices;", None),
        
        # ìƒˆ í…Œì´ë¸” ìƒì„±
        ("""
        CREATE TABLE predicted_stock_prices (
            id SERIAL PRIMARY KEY,
            time TIMESTAMPTZ NOT NULL,
            stock_code VARCHAR(10) NOT NULL,
            stock_name VARCHAR(50) NOT NULL,
            open_price DECIMAL(10,2),
            high_price DECIMAL(10,2),
            low_price DECIMAL(10,2),
            close_price DECIMAL(10,2),
            volume BIGINT,
            market_cap BIGINT,
            foreign_holding BIGINT,
            foreign_holding_ratio DECIMAL(5,2),
            prediction_date TIMESTAMPTZ NOT NULL,
            prediction_confidence DECIMAL(5,2),
            created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
            CONSTRAINT unique_prediction UNIQUE (time, stock_code)
        );
        """, None),
        
        # ì¸ë±ìŠ¤ ìƒì„±
        ("CREATE INDEX idx_predicted_prices_time ON predicted_stock_prices (time);", None),
        ("CREATE INDEX idx_predicted_prices_stock ON predicted_stock_prices (stock_code);", None),
        ("CREATE INDEX idx_predicted_prices_pred_date ON predicted_stock_prices (prediction_date);", None)
    ]
    
    try:
        execute_transaction(queries)
        print("Predicted stock prices table created successfully!")
    except Exception as e:
        print(f"Error creating table: {e}")
        raise

def save_or_update_predicted_price(prediction_data):
    """ì˜ˆì¸¡ ê°€ê²©ì„ ì €ì¥í•˜ê±°ë‚˜ ì—…ë°ì´íŠ¸"""
    query = """
    INSERT INTO predicted_stock_prices (
        time, stock_code, stock_name, open_price, high_price, low_price,
        close_price, volume, market_cap, foreign_holding, foreign_holding_ratio,
        prediction_date, prediction_confidence
    ) VALUES (
        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
    )
    ON CONFLICT (time, stock_code) 
    DO UPDATE SET
        open_price = EXCLUDED.open_price,
        high_price = EXCLUDED.high_price,
        low_price = EXCLUDED.low_price,
        close_price = EXCLUDED.close_price,
        volume = EXCLUDED.volume,
        market_cap = EXCLUDED.market_cap,
        foreign_holding = EXCLUDED.foreign_holding,
        foreign_holding_ratio = EXCLUDED.foreign_holding_ratio,
        prediction_date = EXCLUDED.prediction_date,
        prediction_confidence = EXCLUDED.prediction_confidence,
        updated_at = CURRENT_TIMESTAMP;
    """
    
    try:
        execute_query(query, prediction_data, fetch=False)
        print(f"âœ… ì˜ˆì¸¡ ê°€ê²©ì´ ì €ì¥/ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤. (ë‚ ì§œ: {prediction_data[0]}, ì¢…ëª©: {prediction_data[2]})")
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ê°€ê²© ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

def save_predicted_prices(predictions, dates, stock_code, stock_name, confidence=0.95):
    """ì˜ˆì¸¡ëœ ê°€ê²©ë“¤ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    try:
        # í…Œì´ë¸” ìƒì„± í™•ì¸
        create_predicted_prices_table()
        
        # ê° ì˜ˆì¸¡ ë‚ ì§œì— ëŒ€í•´ ë°ì´í„° ì €ì¥
        for i, (date, pred_price) in enumerate(zip(dates, predictions)):
            # NumPy íƒ€ì…ì„ Python ë„¤ì´í‹°ë¸Œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
            pred_price = float(pred_price)
            
            # ì˜ˆì¸¡ ê°€ê²©ì„ ê¸°ë°˜ìœ¼ë¡œ OHLCV ë°ì´í„° ìƒì„±
            # ì‹¤ì œ ë°ì´í„°ì˜ ë³€ë™ì„±ì„ ê³ ë ¤í•˜ì—¬ ê°€ê²© ë²”ìœ„ ì„¤ì •
            price_volatility = 0.02  # 2% ë³€ë™ì„± ê°€ì •
            volume_volatility = 0.1  # 10% ë³€ë™ì„± ê°€ì •
            
            # ê°€ê²© ë°ì´í„° ìƒì„±
            close_price = pred_price
            high_price = close_price * (1 + price_volatility)
            low_price = close_price * (1 - price_volatility)
            open_price = (high_price + low_price) / 2
            
            # ê±°ë˜ëŸ‰ ë°ì´í„° ìƒì„± (ì´ì „ ê±°ë˜ëŸ‰ì˜ í‰ê· ì„ ê¸°ë°˜ìœ¼ë¡œ)
            volume = int(np.random.normal(1000000, 1000000 * volume_volatility))
            market_cap = int(close_price * volume * 0.1)  # ì‹œê°€ì´ì•¡ ì¶”ì •
            foreign_holding = int(market_cap * 0.3)  # ì™¸êµ­ì¸ ë³´ìœ ëŸ‰ ì¶”ì •
            foreign_holding_ratio = 30.0  # ì™¸êµ­ì¸ ë³´ìœ  ë¹„ìœ¨ ì¶”ì •
            
            # ì˜ˆì¸¡ ë°ì´í„° ì €ì¥
            prediction_data = (
                date,  # time
                stock_code,  # stock_code
                stock_name,  # stock_name
                float(open_price),  # open_price
                float(high_price),  # high_price
                float(low_price),  # low_price
                float(close_price),  # close_price
                int(volume),  # volume
                int(market_cap),  # market_cap
                int(foreign_holding),  # foreign_holding
                float(foreign_holding_ratio),  # foreign_holding_ratio
                datetime.now(),  # prediction_date
                float(confidence)  # prediction_confidence
            )
            
            save_or_update_predicted_price(prediction_data)
            
        print("âœ… ëª¨ë“  ì˜ˆì¸¡ ê°€ê²©ì´ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ê°€ê²© ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

def load_data_from_db():
    """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ"""
    print("Loading stock data...")
    try:
        # í…Œì´ë¸” ìƒì„± í™•ì¸
        create_predictions_table()
        create_economic_indicators_table()
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì£¼ê°€ ë°ì´í„° ë¡œë“œ
        query = """
        SELECT 
            time as ê¸°ì¤€ì¼ì,
            stock_code as ì¢…ëª©ì½”ë“œ,
            stock_name as ì¢…ëª©ëª…,
            open_price as ì‹œê°€,
            high_price as ê³ ê°€,
            low_price as ì €ê°€,
            close_price as í˜„ì¬ê°€,
            volume as ê±°ë˜ëŸ‰,
            market_cap as ì‹œê°€ì´ì•¡,
            foreign_holding as ì™¸êµ­ì¸ë³´ìœ ,
            foreign_holding_ratio as ì™¸êµ­ì¸ë¹„ìœ¨
        FROM stock_prices
        WHERE stock_name = 'LGì „ì'
        ORDER BY time;
        """
        stock_data = pd.DataFrame(execute_query(query), columns=[
            'ê¸°ì¤€ì¼ì', 'ì¢…ëª©ì½”ë“œ', 'ì¢…ëª©ëª…', 'ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 
            'í˜„ì¬ê°€', 'ê±°ë˜ëŸ‰', 'ì‹œê°€ì´ì•¡', 'ì™¸êµ­ì¸ë³´ìœ ', 'ì™¸êµ­ì¸ë¹„ìœ¨'
        ])
        
        # ìˆ«ìí˜• ì»¬ëŸ¼ì„ floatë¡œ ë³€í™˜
        numeric_columns = ['ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'í˜„ì¬ê°€', 'ê±°ë˜ëŸ‰', 'ì‹œê°€ì´ì•¡', 'ì™¸êµ­ì¸ë³´ìœ ', 'ì™¸êµ­ì¸ë¹„ìœ¨']
        for col in numeric_columns:
            stock_data[col] = stock_data[col].astype(float)
        
        print("Stock data columns:", stock_data.columns.tolist())
        print("Stock data shape:", stock_data.shape)
        print("Stock data head:\n", stock_data.head())
        
        # ê°ì„± ë°ì´í„° ë¡œë“œ
        query = """
        SELECT 
            pub_date, title,
            finbert_positive, finbert_negative, finbert_neutral,
            finbert_sentiment
        FROM news_sentiment
        ORDER BY pub_date;
        """
        sentiment_data = pd.DataFrame(execute_query(query), columns=[
            'PubDate', 'Title', 'finbert_positive', 'finbert_negative', 
            'finbert_neutral', 'finbert_sentiment'
        ])
        
        # ê°ì„± ì ìˆ˜ë¥¼ floatë¡œ ë³€í™˜ (finbert_sentiment ì œì™¸)
        sentiment_columns = ['finbert_positive', 'finbert_negative', 'finbert_neutral']
        for col in sentiment_columns:
            sentiment_data[col] = pd.to_numeric(sentiment_data[col], errors='coerce')
        
        # finbert_sentimentë¥¼ ìˆ«ìë¡œ ë§¤í•‘
        sentiment_mapping = {
            'positive': 1.0,
            'negative': -1.0,
            'neutral': 0.0
        }
        sentiment_data['finbert_sentiment'] = sentiment_data['finbert_sentiment'].map(sentiment_mapping)
        
        print("\nSentiment data columns:", sentiment_data.columns.tolist())
        print("Sentiment data shape:", sentiment_data.shape)
        print("Sentiment data head:\n", sentiment_data.head())
        
        # ê²½ì œì§€í‘œ ë°ì´í„° ë¡œë“œ
        query = """
        SELECT 
            time,
            treasury_10y,
            dollar_index,
            usd_krw,
            korean_bond_10y
        FROM economic_indicators
        ORDER BY time;
        """
        economic_data = pd.DataFrame(execute_query(query), columns=[
            'time', 'treasury_10y', 'dollar_index', 'usd_krw', 'korean_bond_10y'
        ])
        
        # ê²½ì œì§€í‘œë¥¼ floatë¡œ ë³€í™˜
        economic_columns = ['treasury_10y', 'dollar_index', 'usd_krw', 'korean_bond_10y']
        for col in economic_columns:
            economic_data[col] = pd.to_numeric(economic_data[col], errors='coerce')
            
        economic_data.set_index('time', inplace=True)
        
        print("\nEconomic data shape:", economic_data.shape)
        print("Economic data head:\n", economic_data.head())
        
        return stock_data, sentiment_data, economic_data
        
    except Exception as e:
        print(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

def save_prediction(stock_code, stock_name, prediction_date, target_date, predicted_price, actual_price=None):
    """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    # NumPy íƒ€ì…ì„ Python ë„¤ì´í‹°ë¸Œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
    predicted_price = float(predicted_price)
    if actual_price is not None:
        actual_price = float(actual_price)
        prediction_error = float(predicted_price - actual_price)
    else:
        prediction_error = None
    
    query = """
    INSERT INTO price_predictions (
        stock_code, stock_name, prediction_date, target_date,
        predicted_price, actual_price, prediction_error
    ) VALUES (%s, %s, %s, %s, %s, %s, %s)
    """
    params = (
        stock_code, stock_name, prediction_date, target_date,
        predicted_price, actual_price, prediction_error
    )
    execute_query(query, params, fetch=False)

# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
print("Loading stock data...")
try:
    # load_data_from_db() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ë°ì´í„°ë¥¼ í•œ ë²ˆì— ë¡œë“œ
    stock_data, sentiment_data, economic_data = load_data_from_db()
    
    print("Stock data columns:", stock_data.columns.tolist())
    print("Stock data shape:", stock_data.shape)
    print("Stock data head:\n", stock_data.head())
    
    print("\nSentiment data columns:", sentiment_data.columns.tolist())
    print("Sentiment data shape:", sentiment_data.shape)
    print("Sentiment data head:\n", sentiment_data.head())
    
    print("\nEconomic indicators data columns:", economic_data.columns.tolist())
    print("Economic indicators data shape:", economic_data.shape)
    print("Economic indicators data head:\n", economic_data.head())
    
except Exception as e:
    print(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    raise

# LGì „ì ë°ì´í„°ë§Œ í•„í„°ë§
lg_data = stock_data[stock_data['ì¢…ëª©ëª…'] == 'LGì „ì'].copy()
print("\nLG data shape:", lg_data.shape)
print("LG data head:\n", lg_data.head())

# ë‚ ì§œ í˜•ì‹ ë³€í™˜
lg_data['ê¸°ì¤€ì¼ì'] = pd.to_datetime(lg_data['ê¸°ì¤€ì¼ì'])
sentiment_data['PubDate'] = pd.to_datetime(sentiment_data['PubDate'])
economic_data.index = pd.to_datetime(economic_data.index)

# ë°ì´í„° ë³‘í•©
merged_data = pd.merge(lg_data, sentiment_data, left_on='ê¸°ì¤€ì¼ì', right_on='PubDate', how='left')
merged_data = pd.merge(merged_data, economic_data, left_on='ê¸°ì¤€ì¼ì', right_index=True, how='left')
print("\nMerged data shape:", merged_data.shape)

# ê¸°ìˆ ì  ì§€í‘œ ì¶”ê°€
def add_technical_indicators(df):
    # RSI
    rsi = RSIIndicator(close=df['í˜„ì¬ê°€'], window=14)
    df['RSI'] = rsi.rsi()

    # MACD
    macd = MACD(close=df['í˜„ì¬ê°€'])
    df['MACD'] = macd.macd()
    df['MACD_SIGNAL'] = macd.macd_signal()
    df['MACD_HIST'] = macd.macd_diff()

    # ë³¼ë¦°ì € ë°´ë“œ
    bbands = BollingerBands(close=df['í˜„ì¬ê°€'], window=20)
    df['BB_UPPER'] = bbands.bollinger_hband()
    df['BB_MIDDLE'] = bbands.bollinger_mavg()
    df['BB_LOWER'] = bbands.bollinger_lband()
    df['BB_PERCENT'] = (df['í˜„ì¬ê°€'] - df['BB_LOWER']) / (df['BB_UPPER'] - df['BB_LOWER'])

    # ì´ë™í‰ê· 
    df['MA5'] = SMAIndicator(close=df['í˜„ì¬ê°€'], window=5).sma_indicator()
    df['MA20'] = SMAIndicator(close=df['í˜„ì¬ê°€'], window=20).sma_indicator()
    df['MA60'] = SMAIndicator(close=df['í˜„ì¬ê°€'], window=60).sma_indicator()

    # ê±°ë˜ëŸ‰ ì§€í‘œ
    df['VOLUME_MA5'] = SMAIndicator(close=df['ê±°ë˜ëŸ‰'], window=5).sma_indicator()
    df['VOLUME_MA20'] = SMAIndicator(close=df['ê±°ë˜ëŸ‰'], window=20).sma_indicator()
    df['VOLUME_RATIO'] = df['ê±°ë˜ëŸ‰'] / df['VOLUME_MA20']

    # ëª¨ë©˜í…€ ì§€í‘œ
    df['MOM'] = df['í˜„ì¬ê°€'].diff(10)
    df['ROC'] = ROCIndicator(close=df['í˜„ì¬ê°€'], window=10).roc()

    return df

# ê¸°ìˆ ì  ì§€í‘œ ì¶”ê°€
merged_data = add_technical_indicators(merged_data)

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
merged_data = merged_data.ffill().bfill().fillna(0)

# ë°ì´í„° ì „ì²˜ë¦¬ ê°œì„ 
def enhanced_preprocessing(df):
    # ê°€ê²© ë³€ë™ë¥  ê³„ì‚°
    df['price_change'] = df['í˜„ì¬ê°€'].pct_change()
    df['price_volatility'] = df['price_change'].rolling(window=5).std()
    
    # ê±°ë˜ëŸ‰ ë³€ë™ë¥ 
    df['volume_change'] = df['ê±°ë˜ëŸ‰'].pct_change()
    df['volume_volatility'] = df['volume_change'].rolling(window=5).std()
    
    # ê°€ê²© ëª¨ë©˜í…€
    df['price_momentum'] = df['í˜„ì¬ê°€'] / df['í˜„ì¬ê°€'].rolling(window=5).mean() - 1
    
    # ê±°ë˜ëŸ‰ ëª¨ë©˜í…€
    df['volume_momentum'] = df['ê±°ë˜ëŸ‰'] / df['ê±°ë˜ëŸ‰'].rolling(window=5).mean() - 1
    
    # ê°€ê²© ë³€ë™ ì¶”ì„¸
    df['price_trend'] = df['í˜„ì¬ê°€'].rolling(window=5).apply(lambda x: np.polyfit(range(len(x)), x, 1)[0])
    
    # ì´ìƒì¹˜ ì²˜ë¦¬ (IQR ë°©ë²•)
    for col in ['í˜„ì¬ê°€', 'ê±°ë˜ëŸ‰', 'price_change', 'volume_change']:
        q1 = df[col].quantile(0.25)
        q3 = df[col].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        df[col] = df[col].clip(lower_bound, upper_bound)
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ìµœì‹  pandas ë°©ì‹)
    df = df.ffill().bfill()
    
    # ê°ì„± ë°ì´í„° ë³´ê°„
    sentiment_cols = ['finbert_positive', 'finbert_negative', 'finbert_neutral']
    for col in sentiment_cols:
        if col in df.columns:
            # ê°ì„± ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë³´ê°„
            mask = df[col] != 0
            if mask.any():
                df[col] = df[col].interpolate(method='linear')
    
    # ê²½ì œ ì§€í‘œ ë³´ê°„
    economic_cols = ['treasury_10y', 'dollar_index', 'usd_krw', 'korean_bond_10y']
    for col in economic_cols:
        if col in df.columns:
            # ê²½ì œ ì§€í‘œê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë³´ê°„
            mask = df[col] != 0
            if mask.any():
                df[col] = df[col].interpolate(method='linear')
    
    return df

# ë°ì´í„° ì „ì²˜ë¦¬ ì ìš©
merged_data = enhanced_preprocessing(merged_data)

# ìŠ¤ì¼€ì¼ë§ í´ë˜ìŠ¤ ê°œì„ 
class EnhancedPriceScaler:
    def __init__(self):
        self.price_scaler = MinMaxScaler()  # RobustScaler ëŒ€ì‹  MinMaxScaler ì‚¬ìš©
        self.feature_scaler = MinMaxScaler()
        self.price_min = None
        self.price_max = None

    def fit_transform(self, data, price_cols):
        data_copy = data.copy()
        
        # ë¬¸ìì—´ ì»¬ëŸ¼ê³¼ ë‚ ì§œ ì»¬ëŸ¼ ì œì™¸
        exclude_cols = ['ê¸°ì¤€ì¼ì', 'ì¢…ëª©ì½”ë“œ', 'ì¢…ëª©ëª…', 'Title', 'PubDate', 'finbert_sentiment']
        for col in exclude_cols:
            if col in data_copy.columns:
                data_copy = data_copy.drop(columns=[col])
        
        # ê°€ê²© ë°ì´í„°ì™€ ë‹¤ë¥¸ íŠ¹ì„± ë¶„ë¦¬
        price_data = data_copy[price_cols]
        other_data = data_copy.drop(columns=price_cols)
        
        # ê°€ê²© ë°ì´í„°ì˜ ìµœì†Œ/ìµœëŒ€ê°’ ì €ì¥
        self.price_min = price_data.min().values
        self.price_max = price_data.max().values
        
        # ê°ê° ìŠ¤ì¼€ì¼ë§
        scaled_price = self.price_scaler.fit_transform(price_data)
        scaled_other = self.feature_scaler.fit_transform(other_data)
        
        # ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° ê²°í•©
        scaled_data = np.concatenate([scaled_price, scaled_other], axis=1)
        scaled_df = pd.DataFrame(scaled_data, columns=price_cols + other_data.columns.tolist())
        
        # ì›ë˜ ì»¬ëŸ¼ ìˆœì„œ ë³µì›
        scaled_df = scaled_df[data_copy.columns]
        
        return scaled_df

    def inverse_transform_price(self, scaled_price):
        if len(scaled_price.shape) == 1:
            scaled_price = scaled_price.reshape(-1, 1)
        
        # MinMaxScaler ì—­ë³€í™˜
        unscaled = self.price_scaler.inverse_transform(scaled_price)
        return unscaled.flatten()

# ì†ì‹¤ í•¨ìˆ˜ ê°œì„ 
@tf.keras.utils.register_keras_serializable()
def enhanced_weighted_time_mse(y_true, y_pred):
    # ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•œ ì‘ì€ ê°’ ì¶”ê°€
    epsilon = 1e-7
    
    # ì‹œê°„ ê°€ì¤‘ì¹˜ ì¡°ì • (ì²«ë‚  ê°€ì¤‘ì¹˜ ê°•í™”)
    time_weights = tf.constant([0.4, 0.3, 0.2, 0.07, 0.03], dtype=tf.float32)
    
    # ê¸°ë³¸ MSE
    mse_per_step = tf.reduce_mean(tf.square(y_true - y_pred) + epsilon, axis=0)
    
    # ê³¼ëŒ€ ì˜ˆì¸¡ íŒ¨ë„í‹° (ì²«ë‚  ê°•í™”)
    overprediction_penalty = tf.reduce_mean(
        tf.maximum(0.0, y_pred - y_true) * tf.constant([25.0, 20.0, 15.0, 10.0, 8.0], dtype=tf.float32)
    )
    
    # ê³¼ì†Œ ì˜ˆì¸¡ íŒ¨ë„í‹° (ì²«ë‚  ê°•í™”)
    underprediction_penalty = tf.reduce_mean(
        tf.maximum(0.0, y_true - y_pred) * tf.constant([10.0, 8.0, 6.0, 5.0, 4.0], dtype=tf.float32)
    )
    
    # ì¶”ì„¸ ì†ì‹¤ (ì²«ë‚  ê°•í™”)
    y_true_diff = y_true[:, 1:] - y_true[:, :-1]
    y_pred_diff = y_pred[:, 1:] - y_pred[:, :-1]
    trend_weights = tf.constant([0.4, 0.3, 0.2, 0.1], dtype=tf.float32)
    trend_loss = tf.reduce_mean(tf.square(y_true_diff - y_pred_diff) * trend_weights + epsilon)
    
    # ë°©í–¥ì„± ì†ì‹¤ (ì²«ë‚  ê°•í™”)
    direction_weights = tf.constant([0.4, 0.3, 0.2, 0.1], dtype=tf.float32)
    direction_loss = tf.reduce_mean(
        tf.square(tf.sign(y_true_diff) - tf.sign(y_pred_diff)) * direction_weights + epsilon
    )
    
    # ì—°ì†ì„± ì†ì‹¤ ì¶”ê°€
    continuity_loss = tf.reduce_mean(
        tf.square(y_pred[:, 1:] - y_pred[:, :-1] - (y_true[:, 1:] - y_true[:, :-1]))
    )
    
    # ê°€ì¤‘ì¹˜ ì ìš©
    weighted_loss = (
        tf.reduce_sum(mse_per_step * time_weights) +
        1.0 * overprediction_penalty +  # ê³¼ëŒ€ ì˜ˆì¸¡ íŒ¨ë„í‹° ë”ìš± ê°•í™”
        0.3 * underprediction_penalty +  # ê³¼ì†Œ ì˜ˆì¸¡ íŒ¨ë„í‹° ê°ì†Œ
        0.5 * trend_loss +
        0.4 * direction_loss +
        0.3 * continuity_loss  # ì—°ì†ì„± ì†ì‹¤ ì¶”ê°€
    )
    return weighted_loss

# ë°ì´í„° ì¦ê°• í•¨ìˆ˜ ê°œì„ 
def augment_data(X, y, noise_level=0.01):
    """ë°ì´í„°ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬ ì¦ê°•"""
    X_aug = X.copy()
    y_aug = y.copy()
    
    # ê°€ê²© ë°ì´í„° ì¶”ì¶œ (ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ê°€ê²©)
    price_data = X[:, :, 0]
    
    # ì¶”ì„¸ ê³„ì‚°
    price_trend = np.diff(price_data, axis=1)
    avg_trend = np.mean(price_trend, axis=1)
    
    # ì¶”ì„¸ ê¸°ë°˜ ë…¸ì´ì¦ˆ ìƒì„±
    trend_noise = np.random.normal(0, noise_level * 0.5, X.shape)
    trend_noise[:, :, 0] *= np.sign(avg_trend)[:, np.newaxis]  # ì¶”ì„¸ ë°©í–¥ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì¡°ì •
    
    # ê¸°ë³¸ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ
    base_noise = np.random.normal(0, noise_level, X.shape)
    
    # ë…¸ì´ì¦ˆ ê²°í•©
    combined_noise = base_noise + trend_noise
    
    # ë…¸ì´ì¦ˆ ì ìš©
    X_aug = X_aug + combined_noise
    
    # ì‹œê³„ì—´ íŠ¹ì„± ë³´ì¡´ì„ ìœ„í•œ ë…¸ì´ì¦ˆ ì œí•œ
    X_aug = np.clip(X_aug, X.min(), X.max())
    
    # ê°€ê²© ì—°ì†ì„± ë³´ì¥
    price_diff = np.diff(X_aug[:, :, 0], axis=1)
    max_allowed_diff = np.std(price_data) * 0.1  # ìµœëŒ€ í—ˆìš© ë³€ë™í­
    price_diff = np.clip(price_diff, -max_allowed_diff, max_allowed_diff)
    
    # ìˆ˜ì •ëœ ê°€ê²© ë°ì´í„° ì¬êµ¬ì„± (ë¸Œë¡œë“œìºìŠ¤íŒ… ìˆ˜ì •)
    initial_prices = X_aug[:, 0, 0][:, np.newaxis]  # (batch_size, 1)
    cumulative_diff = np.cumsum(price_diff, axis=1)  # (batch_size, seq_len-1)
    X_aug[:, 1:, 0] = initial_prices + cumulative_diff
    
    return X_aug, y_aug

# MCI-GRU ì…€ êµ¬í˜„ (cuDNN ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë²„ì „)
@tf.keras.utils.register_keras_serializable()
class MCI_GRU_Cell(Layer):
    def __init__(self, units, return_sequences=False, go_backwards=False, **kwargs):
        super(MCI_GRU_Cell, self).__init__(**kwargs)
        self.units = units
        self.return_sequences = return_sequences
        self.go_backwards = go_backwards
        self.state_size = [units]
        self.output_size = units

    def build(self, input_shape):
        input_dim = input_shape[-1]
        
        # Market Context Integration weights
        self.W_m = self.add_weight(
            shape=(input_dim, self.units),
            name='W_m',
            initializer='glorot_uniform'
        )
        self.U_m = self.add_weight(
            shape=(self.units, self.units),
            name='U_m',
            initializer='glorot_uniform'
        )
        self.b_m = self.add_weight(
            shape=(self.units,),
            name='b_m',
            initializer='zeros'
        )
        
        # GRU gates
        self.W_z = self.add_weight(
            shape=(input_dim, self.units),
            name='W_z',
            initializer='glorot_uniform'
        )
        self.U_z = self.add_weight(
            shape=(self.units, self.units),
            name='U_z',
            initializer='glorot_uniform'
        )
        self.b_z = self.add_weight(
            shape=(self.units,),
            name='b_z',
            initializer='zeros'
        )
        
        self.W_r = self.add_weight(
            shape=(input_dim, self.units),
            name='W_r',
            initializer='glorot_uniform'
        )
        self.U_r = self.add_weight(
            shape=(self.units, self.units),
            name='U_r',
            initializer='glorot_uniform'
        )
        self.b_r = self.add_weight(
            shape=(self.units,),
            name='b_r',
            initializer='zeros'
        )
        
        self.W_h = self.add_weight(
            shape=(input_dim, self.units),
            name='W_h',
            initializer='glorot_uniform'
        )
        self.U_h = self.add_weight(
            shape=(self.units, self.units),
            name='U_h',
            initializer='glorot_uniform'
        )
        self.b_h = self.add_weight(
            shape=(self.units,),
            name='b_h',
            initializer='zeros'
        )
        
        self.built = True

    def call(self, inputs, states=None):
        # Handle states properly
        if states is None:
            states = [tf.zeros((tf.shape(inputs)[0], self.units))]
        else:
            states = list(states)
        
        # Ensure inputs are 3D
        if len(inputs.shape) == 2:
            inputs = tf.expand_dims(inputs, axis=1)
        
        # Handle bidirectional processing
        if self.go_backwards:
            inputs = tf.reverse(inputs, axis=[1])
        
        # Get sequence length
        sequence_length = tf.shape(inputs)[1]
        batch_size = tf.shape(inputs)[0]
        
        # Initialize outputs tensor
        if self.return_sequences:
            outputs = tf.TensorArray(tf.float32, size=sequence_length)
        else:
            outputs = tf.zeros((batch_size, self.units))
        
        # Initialize loop variables
        i = tf.constant(0)
        h = states[0]
        
        # Define loop body
        def body(i, h, outputs):
            # Get current input
            x_t = inputs[:, i, :]
            
            # Market context integration
            m = tf.tanh(
                tf.matmul(x_t, self.W_m) +
                tf.matmul(h, self.U_m) +
                self.b_m
            )
            
            # GRU gates with market context
            z = tf.sigmoid(
                tf.matmul(x_t, self.W_z) +
                tf.matmul(h, self.U_z) +
                self.b_z
            )
            
            r = tf.sigmoid(
                tf.matmul(x_t, self.W_r) +
                tf.matmul(h, self.U_r) +
                self.b_r
            )
            
            h_candidate = tf.tanh(
                tf.matmul(x_t, self.W_h) +
                tf.matmul(r * h, self.U_h) +
                self.b_h
            )
            
            # Update state with market context
            h = z * h + (1 - z) * h_candidate + m
            
            # Store output if return_sequences is True
            if self.return_sequences:
                outputs = outputs.write(i, h)
            
            return i + 1, h, outputs
        
        # Define loop condition
        def cond(i, h, outputs):
            return i < sequence_length
        
        # Run the loop
        _, final_h, final_outputs = tf.while_loop(
            cond=cond,
            body=body,
            loop_vars=[i, h, outputs],
            maximum_iterations=sequence_length
        )
        
        if self.return_sequences:
            # Convert TensorArray to tensor
            outputs = final_outputs.stack()
            # Reshape to [batch, seq_len, units]
            outputs = tf.transpose(outputs, [1, 0, 2])
            if self.go_backwards:
                outputs = tf.reverse(outputs, axis=[1])
            return outputs
        else:
            return final_h

    def get_config(self):
        config = super(MCI_GRU_Cell, self).get_config()
        config.update({
            'units': self.units,
            'return_sequences': self.return_sequences,
            'go_backwards': self.go_backwards
        })
        return config

# ë°ì´í„° ì „ì²˜ë¦¬ ê°œì„ 
def prepare_data(merged_data, sequence_length=30):
    # íŠ¹ì„± ì„ íƒ ìµœì í™”
    feature_columns = [
        'í˜„ì¬ê°€', 'ê±°ë˜ëŸ‰', 'ì‹œê°€ì´ì•¡', 'ì™¸êµ­ì¸ë³´ìœ ', 'ì™¸êµ­ì¸ë¹„ìœ¨',
        'RSI', 'MACD', 'MACD_SIGNAL', 'MACD_HIST',
        'BB_UPPER', 'BB_MIDDLE', 'BB_LOWER', 'BB_PERCENT',
        'MA5', 'MA20', 'MA60', 'VOLUME_MA5', 'VOLUME_MA20',
        'VOLUME_RATIO', 'MOM', 'ROC',
        'finbert_positive', 'finbert_negative', 'finbert_neutral',
        'treasury_10y', 'dollar_index', 'usd_krw', 'korean_bond_10y'
    ]
    
    # ë°ì´í„° ì •ê·œí™”
    scaler = EnhancedPriceScaler()
    price_cols = ['í˜„ì¬ê°€']
    scaled_data = scaler.fit_transform(merged_data[feature_columns], price_cols)
    
    # DataFrameì„ numpy arrayë¡œ ë³€í™˜
    scaled_data = scaled_data.values
    
    # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± (ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹)
    X, y = [], []
    for i in range(len(scaled_data) - sequence_length - 4):
        X.append(scaled_data[i:(i + sequence_length)])
        y.append(scaled_data[i + sequence_length:i + sequence_length + 5, 0])
    
    X = np.array(X)
    y = np.array(y)
    
    # í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í•  (80/10/10)
    train_size = int(len(X) * 0.8)
    val_size = int(len(X) * 0.1)
    
    X_train = X[:train_size]
    y_train = y[:train_size]
    
    X_val = X[train_size:train_size + val_size]
    y_val = y[train_size:train_size + val_size]
    
    X_test = X[train_size + val_size:]
    y_test = y[train_size + val_size:]
    
    return X_train, y_train, X_val, y_val, X_test, y_test, scaler

# ëª¨ë¸ êµ¬ì¡° ê°œì„ 
def build_enhanced_model(input_shape, output_days=5):
    # Input layer
    inputs = Input(shape=input_shape)
    
    # Market Context Layer with attention
    x = MarketContextLayer(input_shape[-1])(inputs)
    
    # Ensure 3D tensor for GRU while preserving sequence length
    x = tf.keras.layers.Reshape((input_shape[0], input_shape[-1]))(x)
    
    # Trend Detection Layer
    trend_conv = Conv1D(32, kernel_size=3, padding='same', activation='relu')(x)
    trend_conv = BatchNormalization()(trend_conv)
    trend_conv = Dropout(0.2)(trend_conv)
    
    # Bidirectional GRU Layer with trend information
    x = tf.keras.layers.Bidirectional(
        GRU(input_shape[-1], return_sequences=True)
    )(x)
    x = Dropout(0.2)(x)
    
    # Combine trend and GRU outputs
    x = tf.keras.layers.Concatenate()([x, trend_conv])
    
    # Temporal Attention Layer
    attention_output = MultiHeadAttention(
        num_heads=4, 
        key_dim=input_shape[-1]
    )(x, x)
    x = tf.keras.layers.Add()([x, attention_output])
    x = BatchNormalization()(x)
    
    # Global Average Pooling
    x = tf.keras.layers.GlobalAveragePooling1D()(x)
    
    # Dense layers with residual connections
    dense1 = Dense(64, activation='relu')(x)
    dense1 = BatchNormalization()(dense1)
    dense1 = Dropout(0.2)(dense1)
    
    dense2 = Dense(32, activation='relu')(dense1)
    dense2 = BatchNormalization()(dense2)
    dense2 = Dropout(0.2)(dense2)
    
    # Residual connection
    residual = Dense(32)(x)
    x = tf.keras.layers.Add()([dense2, residual])
    
    # Trend-specific output layers
    outputs = []
    for i in range(output_days):
        # Trend-specific dense layer
        trend_dense = Dense(16, activation='relu')(x)
        trend_dense = BatchNormalization()(trend_dense)
        
        # Price prediction layer
        day_output = Dense(8, activation='relu')(trend_dense)
        day_output = BatchNormalization()(day_output)
        day_output = Dense(1, name=f'day_{i+1}_output')(day_output)
        outputs.append(day_output)
    
    final_output = tf.keras.layers.Concatenate()(outputs)
    
    # Create model
    model = Model(inputs=inputs, outputs=final_output)
    
    # Compile model with adjusted learning rate and loss function
    optimizer = Adam(learning_rate=0.0001)
    model.compile(
        optimizer=optimizer,
        loss=enhanced_weighted_time_mse,
        metrics=['mae', 'mse']
    )
    
    return model

@tf.keras.utils.register_keras_serializable()
class MarketContextLayer(Layer):
    def __init__(self, units, **kwargs):
        super(MarketContextLayer, self).__init__(**kwargs)
        self.units = units
        self.attention = MultiHeadAttention(num_heads=4, key_dim=units)
        self.dense = Dense(units)
        self.batch_norm = BatchNormalization()
        
    def call(self, inputs):
        # Ensure inputs are 3D for attention
        if len(inputs.shape) == 2:
            inputs = tf.expand_dims(inputs, axis=1)
            
        # Self-attention for market context
        attention_output = self.attention(inputs, inputs)
        
        # Dense transformation
        x = self.dense(attention_output)
        x = self.batch_norm(x)
        
        return x
    
    def get_config(self):
        config = super(MarketContextLayer, self).get_config()
        config.update({'units': self.units})
        return config

# í•™ìŠµ ê³¼ì • ê°œì„ 
def train_enhanced_model(model, X_train, y_train, X_val, y_val):
    # ì½œë°± ì •ì˜
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=20,
            restore_best_weights=True,
            min_delta=0.0001
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=10,
            min_lr=1e-6,
            min_delta=0.0001
        ),
        tf.keras.callbacks.ModelCheckpoint(
            '/kaggle/working/best_model.keras',
            monitor='val_loss',
            save_best_only=True,
            save_weights_only=False
        )
    ]
    
    # ë°ì´í„° ì¦ê°• ì ìš© (reduced noise)
    X_train_aug, y_train_aug = augment_data(X_train, y_train, noise_level=0.00005)
    
    # ë°ì´í„°ì…‹ ìµœì í™”
    train_dataset = tf.data.Dataset.from_tensor_slices((X_train_aug, y_train_aug))
    train_dataset = train_dataset.shuffle(buffer_size=1000)
    train_dataset = train_dataset.batch(16)  # Smaller batch size
    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)
    
    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))
    val_dataset = val_dataset.batch(16)
    val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)
    
    # í•™ìŠµ
    history = model.fit(
        train_dataset,
        validation_data=val_dataset,
        epochs=250,  # Increased epochs
        callbacks=callbacks,
        verbose=1
    )
    
    return history

# ì•™ìƒë¸” ëª¨ë¸ í´ë˜ìŠ¤ ê°œì„ 
class EnsembleModel:
    def __init__(self, input_shape, n_models=5):  # ëª¨ë¸ ìˆ˜ ì¦ê°€
        self.models = []
        self.input_shape = input_shape
        self.n_models = n_models
        self.model_weights = None
        
    def build_models(self):
        for i in range(self.n_models):
            model = build_enhanced_model(self.input_shape)
            self.models.append(model)
    
    def train(self, X_train, y_train, X_val, y_val):
        histories = []
        val_losses = []
        
        # ê° ëª¨ë¸ í•™ìŠµ
        for i, model in enumerate(self.models):
            print(f"\nTraining model {i+1}/{self.n_models}")
            
            # ë°ì´í„° ì¦ê°• ê°•í™”
            X_train_aug, y_train_aug = augment_data(X_train, y_train, noise_level=0.0005 * (i + 1))
            
            history = train_enhanced_model(model, X_train_aug, y_train_aug, X_val, y_val)
            histories.append(history)
            
            # ê²€ì¦ ì†ì‹¤ ì €ì¥
            val_loss = min(history.history['val_loss'])
            val_losses.append(val_loss)
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            tf.keras.backend.clear_session()
            
            # ëª¨ë¸ ì €ì¥ ë° ë©”ëª¨ë¦¬ í•´ì œ
            model.save(f'/kaggle/working/model_{i+1}.keras')
            del model
            tf.keras.backend.clear_session()
        
        # ëª¨ë¸ ê°€ì¤‘ì¹˜ ê³„ì‚°
        val_losses = np.array(val_losses)
        self.model_weights = 1.0 / (val_losses + 1e-7)
        self.model_weights = self.model_weights / np.sum(self.model_weights)
        
        return histories
    
    def predict(self, X):
        predictions = []
        for i in range(self.n_models):
            # ëª¨ë¸ ë¡œë“œ
            model = tf.keras.models.load_model(
                f'/kaggle/working/model_{i+1}.keras',
                custom_objects={
                    'MCI_GRU_Cell': MCI_GRU_Cell
                }
            )
            
            # ì˜ˆì¸¡
            pred = model.predict(X)
            predictions.append(pred * self.model_weights[i])
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del model
            tf.keras.backend.clear_session()
            
        return np.sum(predictions, axis=0)

# ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰
print("ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
X_train, y_train, X_val, y_val, X_test, y_test, scaler = prepare_data(merged_data)
print("ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ")
print(f"X_train shape: {X_train.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"X_val shape: {X_val.shape}")
print(f"y_val shape: {y_val.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_test shape: {y_test.shape}")

# ì•™ìƒë¸” ëª¨ë¸ ì‚¬ìš©
ensemble = EnsembleModel(input_shape=(X_train.shape[1], X_train.shape[2]))
ensemble.build_models()
histories = ensemble.train(X_train, y_train, X_val, y_val)

# ì˜ˆì¸¡ ìˆ˜í–‰
predictions = ensemble.predict(X_test)

# í•™ìŠµ ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(histories[0].history['loss'], label='Training Loss')
plt.plot(histories[0].history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(histories[0].history['mae'], label='Training MAE')
plt.plot(histories[0].history['val_mae'], label='Validation MAE')
plt.title('Model MAE')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend()

plt.tight_layout()
plt.show()

# ëª¨ë¸ ì €ì¥
for i, model in enumerate(ensemble.models):
    model.save(f'/kaggle/working/stock_prediction_model_{i+1}.keras')  # .h5 ëŒ€ì‹  .keras ì‚¬ìš©
    
# ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
with open(f'/kaggle/working/stock_prediction_scaler_{i+1}.pkl', 'wb') as f:
    pickle.dump(scaler, f)
    
# ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì €ì¥
model_metadata = {
    'input_shape': X_train.shape[1:],
    'output_days': 5,
    'price_scaler_params': {
        'scale_': scaler.price_scaler.scale_.tolist(),
        'min_': scaler.price_scaler.min_.tolist(),
        'data_min_': scaler.price_scaler.data_min_.tolist(),
        'data_max_': scaler.price_scaler.data_max_.tolist(),
        'feature_names_in_': scaler.price_scaler.feature_names_in_.tolist()
    }
}
    
with open(f'/kaggle/working/model_metadata_{i+1}.json', 'w') as f:
    json.dump(model_metadata, f)
        
print(f"ëª¨ë¸ {i+1} ì €ì¥ ì™„ë£Œ")
print(f"ìŠ¤ì¼€ì¼ëŸ¬ {i+1} ì €ì¥ ì™„ë£Œ")
print(f"ë©”íƒ€ë°ì´í„° {i+1} ì €ì¥ ì™„ë£Œ")

# ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”
try:
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰
    predictions = ensemble.predict(X_test)

    # ë§ˆì§€ë§‰ ì˜ˆì¸¡ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (ê°€ì¥ ìµœê·¼ ì˜ˆì¸¡)
    last_prediction = predictions[-1]

    # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
    last_prediction = scaler.inverse_transform_price(last_prediction.reshape(-1, 1)).flatten()

    # ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ ë¹„êµ
    target_dates = ['2025-03-24', '2025-03-25', '2025-03-26', '2025-03-27', '2025-03-28']
    target_prices = [69700, 67500, 67200, 66800, 65700]

    # ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
    plt.figure(figsize=(12, 6))

    # ë‚ ì§œë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜
    dates = [datetime.strptime(date, '%Y-%m-%d') for date in target_dates]

    # ì‹¤ì œ ê°€ê²©ê³¼ ì˜ˆì¸¡ ê°€ê²© í”Œë¡¯
    plt.plot(dates, target_prices, 'b-', label='ì‹¤ì œ ê°€ê²©', marker='o')
    plt.plot(dates, last_prediction, 'r--', label='ì˜ˆì¸¡ ê°€ê²©', marker='s')

    # ê·¸ë˜í”„ ìŠ¤íƒ€ì¼ë§
    plt.title('LGì „ì ì£¼ê°€ ì˜ˆì¸¡ ê²°ê³¼ (2025ë…„ 3ì›”)', fontsize=14)
    plt.xlabel('ë‚ ì§œ', fontsize=12)
    plt.ylabel('ì£¼ê°€ (ì›)', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)

    # xì¶• ë‚ ì§œ í¬ë§· ì„¤ì •
    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    plt.gca().xaxis.set_major_locator(mdates.DayLocator())
    plt.xticks(rotation=45)

    # ì˜¤ì°¨ìœ¨ ê³„ì‚° ë° í‘œì‹œ
    error_rates = [(pred - actual) / actual * 100 for pred, actual in zip(last_prediction, target_prices)]
    for i, (date, error) in enumerate(zip(dates, error_rates)):
        plt.annotate(f'{error:.2f}%',
                    xy=(date, max(last_prediction[i], target_prices[i])),
                    xytext=(0, 10),
                    textcoords='offset points',
                    ha='center',
                    fontsize=10)

    plt.tight_layout()
    plt.show()

    # ì˜ˆì¸¡ ê²°ê³¼ ìƒì„¸ ë¶„ì„
    print("\n[ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„]")
    print(f"{'ë‚ ì§œ':<12} {'ì‹¤ì œ ê°€ê²©':>10} {'ì˜ˆì¸¡ ê°€ê²©':>10} {'ì˜¤ì°¨ìœ¨':>8}")
    print("-" * 45)
    for date, actual, pred, error in zip(target_dates, target_prices, last_prediction, error_rates):
        print(f"{date:<12} {actual:>10,d} {pred:>10.0f} {error:>7.2f}%")

    # ì „ì²´ ì˜ˆì¸¡ ì„±ëŠ¥ ì§€í‘œ
    mae = mean_absolute_error(target_prices, last_prediction)
    mse = mean_squared_error(target_prices, last_prediction)
    rmse = np.sqrt(mse)
    mape = np.mean(np.abs(error_rates))

    print("\n[ì „ì²´ ì˜ˆì¸¡ ì„±ëŠ¥]")
    print(f"MAE: {mae:.2f}")
    print(f"RMSE: {rmse:.2f}")
    print(f"MAPE: {mape:.2f}%")

    # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
    for i, (date, pred, actual) in enumerate(zip(target_dates, last_prediction, target_prices)):
        save_prediction(
            stock_code='066570',  # LGì „ì ì¢…ëª©ì½”ë“œ
            stock_name='LGì „ì',
            prediction_date=datetime.now(),
            target_date=datetime.strptime(date, '%Y-%m-%d'),
            predicted_price=pred,
            actual_price=actual
        )
    
    print("âœ… ì˜ˆì¸¡ ê²°ê³¼ê°€ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ (predicted_stock_prices í…Œì´ë¸”)
    try:
        # ì˜ˆì¸¡ ë‚ ì§œ ìƒì„± (target_dates ì‚¬ìš©)
        prediction_dates = [datetime.strptime(date, '%Y-%m-%d') for date in target_dates]
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        save_predicted_prices(
            predictions=last_prediction,
            dates=prediction_dates,
            stock_code='066570',  # LGì „ì ì¢…ëª©ì½”ë“œ
            stock_name='LGì „ì',
            confidence=0.95
        )
        
    except Exception as e:
        print(f"ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

except Exception as e:
    print(f"ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    raise

if __name__ == "__main__":
    print("ğŸ“¢ ì£¼ê°€ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    create_predictions_table()
    
    # ë°ì´í„° ë¡œë“œ
    stock_data, sentiment_data, economic_data = load_data_from_db()
    
    # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
    for i, (date, pred, actual) in enumerate(zip(target_dates, last_prediction, target_prices)):
        save_prediction(
            stock_code='066570',  # LGì „ì ì¢…ëª©ì½”ë“œ
            stock_name='LGì „ì',
            prediction_date=datetime.now(),
            target_date=datetime.strptime(date, '%Y-%m-%d'),
            predicted_price=pred,
            actual_price=actual
        )
    
    print("âœ… ì˜ˆì¸¡ ê²°ê³¼ê°€ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")